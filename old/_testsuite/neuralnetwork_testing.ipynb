{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d (1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d (6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120)\n",
      "  (fc2): Linear(in_features=120, out_features=84)\n",
      "  (fc3): Linear(in_features=84, out_features=10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# lets code a simple feed forward neural network\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # 6 input channels, 16 output channels, 5x5 square convolution kernel\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # we also need some linear transformations; these are initialised by their in_ and out_features\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 16 output ch from second layer times the kernel size \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)  \n",
    "        # I don't know, where 120, 84 and 10 come from; but they stem from the picture in the tutorial\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        # => from a 2x2 window take the max value. relu = Rectified Linear unit; relu(x) = max(0,x)\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(x.size(0), -1)  # this function replaced the num_flat_features from the tutorial\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we created a ```forward``` function, a ```backward``` function will be created automagically. The backward function can then be used for \n",
    "``` autograd ```. \n",
    "\n",
    "learnable parameters of a model: ```net.parameters()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.0422  0.1326 -0.1497  0.1332 -0.1435\n",
      " -0.0981  0.1486 -0.0478  0.1822 -0.1259\n",
      " -0.0827 -0.0105 -0.1260 -0.0991 -0.0467\n",
      "  0.1747  0.1547 -0.0756  0.1781  0.0076\n",
      "  0.1063 -0.1987 -0.0100  0.0198 -0.0666\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      " -0.0246 -0.1099  0.1335 -0.1048 -0.0662\n",
      "  0.1759 -0.1175 -0.0679 -0.0134  0.0980\n",
      " -0.0322  0.1630  0.1893  0.1355 -0.0755\n",
      "  0.0227  0.1194  0.1236  0.1811  0.1407\n",
      " -0.1081  0.1927  0.1280  0.0220 -0.0808\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      " -0.0387  0.0119  0.1786 -0.0696 -0.0710\n",
      " -0.0696 -0.1785  0.1896 -0.0441  0.0579\n",
      " -0.1013  0.1831 -0.1188  0.0047  0.1406\n",
      "  0.0967 -0.1223  0.1321 -0.0568 -0.1077\n",
      "  0.1127 -0.1153  0.1979 -0.1258 -0.0429\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      " -0.0280  0.1213  0.1458 -0.0799  0.1522\n",
      "  0.1565 -0.1184 -0.0131 -0.1579  0.1548\n",
      "  0.0861 -0.1382  0.1314 -0.1928 -0.0272\n",
      "  0.1141  0.1184 -0.0622  0.0567 -0.1393\n",
      " -0.1454  0.1954 -0.1382  0.0198 -0.1250\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "  0.0300 -0.1286 -0.1265  0.0209  0.1932\n",
      "  0.0406  0.0997  0.0256 -0.1047 -0.1553\n",
      "  0.1907 -0.0753 -0.0045  0.1957 -0.1610\n",
      " -0.1482  0.0153  0.1401  0.1797  0.1970\n",
      "  0.0936  0.0028  0.0800 -0.1114  0.1296\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "  0.1469  0.0425 -0.0866 -0.0989  0.1065\n",
      " -0.0525 -0.0223 -0.1536 -0.0385  0.1009\n",
      "  0.1123 -0.0659 -0.0301 -0.1455  0.1012\n",
      "  0.1088 -0.0387 -0.0993  0.1054 -0.0005\n",
      "  0.1285 -0.0361  0.0980  0.0098 -0.1557\n",
      "[torch.FloatTensor of size 6x1x5x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(params[0])\n",
    "# NOTE: I don't know exactly what these weights correspond to in the actual network above. \n",
    "# In the sense of: how does the network look like, where do these weights actually sit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to the ```forward``` method is an ```autograd.Variable```. The produced output is of the same type. \n",
    "The CNN expects an input of $32 \\times 32$ picture sizes. how does that work? let's walk through that:\n",
    "\n",
    "First, let us just assume for the moment, that we have a $32\\times32\\times1$ sized picture (we only have one colour channel, hence BW picture). Our first layer ```net.conv1``` has a $5\\times5$ convolutional layer, with kernel size (KS) of $5$ and $6$ outputs $\\Leftarrow 6$ kernels.\n",
    "\n",
    "The default values in ```torch.nn.Conv2d``` for ```stride``` and ```padding``` are $1$ and $0$ respectively. Since we didn't set any padding ourselves, we'll lose some information of the image. If the function\n",
    "```python\n",
    "out = lambda wid,KS,pad,st: ((wid-KS+2*pad)/st+1)\n",
    "```\n",
    "with wid=width, KS=kernel size, pad=padding, st=stride, returns an integer, we don't lose information. In our case we have ```out(32, 5, 0, 1)=13.5```.\n",
    "To ensure, that we don't lose information, we can set $\\mathrm{pad} = (\\mathrm{KS}-1)/2$, if we additionally make sure that $\\mathrm{st} = 1$.\n",
    "\n",
    "The loss of information is $\\mathrm{KS} -1$, so after ```net.conv1``` we end up with $28\\times28\\times6$, because we have 6 outputs. This is fed into the 2d pooling layer of $2\\times2$, so we get $14\\times14\\times6$, because channels aren't pooled.\n",
    "Same procedure again with layer ```net.conv2``` $\\Rightarrow\\ 10\\times10\\times16$, and after pooling $5\\times5\\times16$.\n",
    "\n",
    "This output tensor is then streched into a 1d object, such that ```net.fc1``` is able to use it. Hence the $16\\cdot 5 \\cdot 5$ input size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## actual input, now!\n",
    "\n",
    "Our input is going to be a ```autograd.Variable```, such that all the wonderful automagical stuff happens in the background. :-)\n",
    "```Variable``` takes a $4$-dimensional tensor: $\\mathrm{nSamples} \\times \\mathrm{nChannels}\\times \\mathrm{height}\\times \\mathrm{width}$.\n",
    "\n",
    "```torch.nn``` only supports mini-batches, no single samples. So if we had only a single sample, we'd have to use ```inp.unsqueeze(0)``` to add a fake bath dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.0533  0.0633  0.0528 -0.0811 -0.0318 -0.0764  0.0818 -0.1272 -0.1108 -0.0568\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inp = Variable(torch.randn(1, 1, 32, 32))  \n",
    "# what is a channel? think of an RGB picture - it has 3 channels, each for a certain colour. each channel has info\n",
    "# about the whole picture\n",
    "out = net(inp)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to _zero_ the gradient buffers of all parameters and backpropagate using random gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## going further, loss functions, yeeehaw!\n",
    "\n",
    "Simple loss function: ```nn.MSELOSS```, which computes the Mean-Squared Error between input and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 39.0502\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = net(inp)\n",
    "target = Variable(torch.arange(1, 11))  # a dummy target, for example\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## and now for something completely backprop\n",
    "\n",
    "to use backpropagation we just have to use the ```.backward()``` method for the loss, which is automagically there. We need to clear existing gradients, otherwise the new gradients will be accumulated to the existing gradients.\n",
    "\n",
    "We'll inspect the biases of ```net.conv1``` before and after the backprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: conv1.bias.grad before backward\n",
      "Variable containing:\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 6]\n",
      "\n",
      ":: conv1.bias.grad after backward\n",
      "Variable containing:\n",
      "-0.0060\n",
      " 0.0450\n",
      "-0.0926\n",
      " 0.0781\n",
      " 0.1819\n",
      "-0.0768\n",
      "[torch.FloatTensor of size 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clear gradients:\n",
    "net.zero_grad()\n",
    "\n",
    "print(':: conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(':: conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, the only thing that's left is...\n",
    "## weight update\n",
    "\n",
    "which is done the easiest by using the stochastic gradient descent (SGD). The procedure is simple:\n",
    "```python\n",
    "weight = weight - learning_rate * gradient\n",
    "```\n",
    "\n",
    "SIDE NOTE: I'm more or less copy-pasting text & code from the tutorial here, and I think in the above they just meant to write _Gradient Descent_, because there is no stochasticity in the above formula. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To utilize more update rules, without the need to code them ourselves, we can use the ```torch.optim``` package. It is used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(inp)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test section\n",
    "below is just some testing of the above stuff, without any real reason. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "955 µs ± 5.66 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "896 µs ± 2.55 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# test function to get an idea, how long it takes for a network to do a full data propagation with weight adaption\n",
    "# still to add: weight adaption with learning rate, loss function, ..\n",
    "def test(net):\n",
    "    input = Variable(torch.randn(1,1,32,32))\n",
    "    out = net(input)\n",
    "    net.zero_grad()\n",
    "    out.backward(torch.randn(1,10))\n",
    "%timeit test(net)\n",
    "\n",
    "def test2(net):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = net(inp)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step() \n",
    "%timeit test2(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0544  0.0016  0.1104  0.0930  0.1158 -0.0569  0.0511  0.1915  0.1072  0.0430\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
